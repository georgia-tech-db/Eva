{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4df227",
   "metadata": {},
   "source": [
    "# Similarity search for motif mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29b79c",
   "metadata": {},
   "source": [
    "In this tutorial, we demonstrate how to utilize the similarity functionality to discover images with similar motifs from a collection of Reddit images. We employ the classic `SIFT` feature to identify images with a strikingly similar appearance (image-level pipeline).\n",
    "\n",
    "Additionally, we extend the pipeline by incorporating an object detection model, `YOLO`, in combination with the SIFT feature. This enables us to identify objects within the images that exhibit a similar appearance (object-level similarity).\n",
    "\n",
    "To illustrate the seamless integration of different vector stores, we leverage the power of multiple vector stores, namely `FAISS` and `QDRANT`, within evadb. This demonstrates the ease with which you can utilize diverse vector stores to construct indexes, enhancing your similarity search experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df3057",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/georgia-tech-db/eva/blob/master/tutorials/11-similarity-search-for-motif-mining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run on Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/georgia-tech-db/eva/blob/master/tutorials/11-similarity-search-for-motif-mining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/georgia-tech-db/eva/raw/master/tutorials/11-similarity-search-for-motif-mining.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /> Download notebook</a>\n",
    "  </td>\n",
    "</table><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1674e",
   "metadata": {},
   "source": [
    "### Connect to EvaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5181dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T18:45:07.018745Z",
     "iopub.status.busy": "2023-09-05T18:45:07.018406Z",
     "iopub.status.idle": "2023-09-05T18:45:12.878912Z",
     "shell.execute_reply": "2023-09-05T18:45:12.878535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"evadb[vision,document,notebook]\"\n",
    "%pip install --quiet kornia qdrant_client\n",
    "import evadb\n",
    "cursor = evadb.connect().cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f4144",
   "metadata": {},
   "source": [
    "### Download reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527ec1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T18:45:12.880768Z",
     "iopub.status.busy": "2023-09-05T18:45:12.880525Z",
     "iopub.status.idle": "2023-09-05T18:45:13.152940Z",
     "shell.execute_reply": "2023-09-05T18:45:13.152436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘reddit-images.zip’ already there; not retrieving.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  reddit-images.zip\r\n",
      "warning:  stripped absolute path spec from /\r\n",
      "mapname:  conversion of  failed\r\n",
      " extracting: reddit-images/g348_d7jgzgf.jpg  \r\n",
      " extracting: reddit-images/g348_d7jphyc.jpg  \r\n",
      " extracting: reddit-images/g348_d7ju7dq.jpg  \r\n",
      " extracting: reddit-images/g348_d7jhhs3.jpg  \r\n",
      " extracting: reddit-images/g1074_d4n1lmn.jpg  \r\n",
      " extracting: reddit-images/g1074_d4mxztt.jpg  \r\n",
      " extracting: reddit-images/g1074_d4n60oy.jpg  \r\n",
      " extracting: reddit-images/g1074_d4n6fgs.jpg  \r\n",
      " extracting: reddit-images/g1190_cln9xzr.jpg  \r\n",
      " extracting: reddit-images/g1190_cln97xm.jpg  \r\n",
      " extracting: reddit-images/g1190_clna260.jpg  \r\n",
      " extracting: reddit-images/g1190_clna2x2.jpg  \r\n",
      " extracting: reddit-images/g1190_clna91w.jpg  \r\n",
      " extracting: reddit-images/g1190_clnad42.jpg  \r\n",
      " extracting: reddit-images/g1190_clnajd7.jpg  \r\n",
      " extracting: reddit-images/g1190_clnapoy.jpg  \r\n",
      " extracting: reddit-images/g1190_clnarjl.jpg  \r\n",
      " extracting: reddit-images/g1190_clnavnu.jpg  \r\n",
      " extracting: reddit-images/g1190_clnbalu.jpg  \r\n",
      " extracting: reddit-images/g1190_clnbf07.jpg  \r\n",
      " extracting: reddit-images/g1190_clnc4uy.jpg  \r\n",
      " extracting: reddit-images/g1190_clncot0.jpg  \r\n",
      " extracting: reddit-images/g1190_clndsnu.jpg  \r\n",
      " extracting: reddit-images/g1190_clnce4b.jpg  \r\n",
      " extracting: reddit-images/g1209_ct65pvl.jpg  \r\n",
      " extracting: reddit-images/g1209_ct66erw.jpg  \r\n",
      " extracting: reddit-images/g1209_ct67oqk.jpg  \r\n",
      " extracting: reddit-images/g1209_ct6a0g5.jpg  \r\n",
      " extracting: reddit-images/g1209_ct6bf1n.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3o1h6.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3om3h.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3qysz.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3r4gw.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3z7jw.jpg  \r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://www.dropbox.com/scl/fo/fcj6ojmii0gw92zg3jb2s/h\\?dl\\=1\\&rlkey\\=j3kj1ox4yn5fhonw06v0pn7r9 -O reddit-images.zip\n",
    "!unzip -o reddit-images.zip -d reddit-images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c9917",
   "metadata": {},
   "source": [
    "### Load all images into evadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9bca7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T18:45:13.154859Z",
     "iopub.status.busy": "2023-09-05T18:45:13.154731Z",
     "iopub.status.idle": "2023-09-05T18:45:14.549854Z",
     "shell.execute_reply": "2023-09-05T18:45:14.549522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number of loaded IMAGE: 34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "0  Number of loaded IMAGE: 34"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.query(\"DROP TABLE IF EXISTS reddit_dataset;\").df()\n",
    "cursor.query(\"LOAD IMAGE 'reddit-images/*.jpg' INTO reddit_dataset;\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743684c",
   "metadata": {},
   "source": [
    "### Register a SIFT FeatureExtractor \n",
    "It uses `kornia` library to extract sift features for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49496e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T18:45:14.551360Z",
     "iopub.status.busy": "2023-09-05T18:45:14.551250Z",
     "iopub.status.idle": "2023-09-05T18:45:14.879020Z",
     "shell.execute_reply": "2023-09-05T18:45:14.878545Z"
    }
   },
   "outputs": [
    {
     "ename": "UnexpectedToken",
     "evalue": "Unexpected token Token('ID', 'FUNCTION') at line 1, column 6.\nExpected one of: \n\t* INDEX\n\t* UDF\n\t* DATABASE\n\t* TABLE\nPrevious tokens: [Token('DROP', 'DROP')]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedCharacters\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/lexer.py:648\u001b[0m, in \u001b[0;36mContextualLexer.lex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    647\u001b[0m         lexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexers[parser_state\u001b[38;5;241m.\u001b[39mposition]\n\u001b[0;32m--> 648\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mlexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/lexer.py:581\u001b[0m, in \u001b[0;36mBasicLexer.next_token\u001b[0;34m(self, lex_state, parser_state)\u001b[0m\n\u001b[1;32m    580\u001b[0m         allowed \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<END-OF-FILE>\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedCharacters(lex_state\u001b[38;5;241m.\u001b[39mtext, line_ctr\u001b[38;5;241m.\u001b[39mchar_pos, line_ctr\u001b[38;5;241m.\u001b[39mline, line_ctr\u001b[38;5;241m.\u001b[39mcolumn,\n\u001b[1;32m    582\u001b[0m                                allowed\u001b[38;5;241m=\u001b[39mallowed, token_history\u001b[38;5;241m=\u001b[39mlex_state\u001b[38;5;241m.\u001b[39mlast_token \u001b[38;5;129;01mand\u001b[39;00m [lex_state\u001b[38;5;241m.\u001b[39mlast_token],\n\u001b[1;32m    583\u001b[0m                                state\u001b[38;5;241m=\u001b[39mparser_state, terminals_by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminals_by_name)\n\u001b[1;32m    585\u001b[0m value, type_ \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mUnexpectedCharacters\u001b[0m: No terminal matches 'F' in the current parser context, at line 1 col 6\n\nDROP FUNCTION IF EXISTS SiftFeatureExtractor;\n     ^\nExpected one of: \n\t* UDF\n\t* INDEX\n\t* DATABASE\n\t* TABLE\n\nPrevious tokens: Token('DROP', 'DROP')\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnexpectedToken\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP FUNCTION IF EXISTS SiftFeatureExtractor;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdf()\n\u001b[1;32m      2\u001b[0m cursor\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    CREATE FUNCTION SiftFeatureExtractor\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    IMPL \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../evadb/functions/sift_feature_extractor.py\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdf()\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/evadb/interfaces/relational/db.py:443\u001b[0m, in \u001b[0;36mEvaDBCursor.query\u001b[0;34m(self, sql_query)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, sql_query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EvaDBQuery:\n\u001b[1;32m    426\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    Executes a SQL query.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m        2     5     6\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     stmt \u001b[38;5;241m=\u001b[39m \u001b[43mparse_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EvaDBQuery(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evadb, stmt)\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/evadb/parser/utils.py:147\u001b[0m, in \u001b[0;36mparse_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_query\u001b[39m(query):\n\u001b[0;32m--> 147\u001b[0m     stmt \u001b[38;5;241m=\u001b[39m \u001b[43mParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stmt) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stmt[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/evadb/parser/parser.py:38\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self, query_string)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     lark_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lark_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lark_output\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/evadb/parser/lark_parser.py:49\u001b[0m, in \u001b[0;36mLarkParser.parse\u001b[0;34m(self, query_string)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query_string\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     47\u001b[0m     query_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m output \u001b[38;5;241m=\u001b[39m LarkInterpreter(query_string)\u001b[38;5;241m.\u001b[39mvisit(tree)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# convert output to list if it is a single element\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/lark.py:652\u001b[0m, in \u001b[0;36mLark.parse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, start: Optional[\u001b[38;5;28mstr\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, on_error: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptional[Callable[[UnexpectedInput], bool]]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParseTree\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the given text, according to the options provided.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/parser_frontends.py:101\u001b[0m, in \u001b[0;36mParsingFrontend.parse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m     99\u001b[0m kw \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_error\u001b[39m\u001b[38;5;124m'\u001b[39m: on_error}\n\u001b[1;32m    100\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_lexer_thread(text)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:41\u001b[0m, in \u001b[0;36mLALR_Parser.parse\u001b[0;34m(self, lexer, start, on_error)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, lexer, start, on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedInput \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:171\u001b[0m, in \u001b[0;36m_Parser.parse\u001b[0;34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_interactive:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InteractiveParser(\u001b[38;5;28mself\u001b[39m, parser_state, parser_state\u001b[38;5;241m.\u001b[39mlexer)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:193\u001b[0m, in \u001b[0;36m_Parser.parse_from_state\u001b[0;34m(self, state, last_token)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:183\u001b[0m, in \u001b[0;36m_Parser.parse_from_state\u001b[0;34m(self, state, last_token)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     token \u001b[38;5;241m=\u001b[39m last_token\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m state\u001b[38;5;241m.\u001b[39mlexer\u001b[38;5;241m.\u001b[39mlex(state):\n\u001b[1;32m    184\u001b[0m         state\u001b[38;5;241m.\u001b[39mfeed_token(token)\n\u001b[1;32m    186\u001b[0m     end_token \u001b[38;5;241m=\u001b[39m Token\u001b[38;5;241m.\u001b[39mnew_borrow_pos(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$END\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, token) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;28;01melse\u001b[39;00m Token(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$END\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/eva/evadb_venv/lib/python3.11/site-packages/lark/lexer.py:657\u001b[0m, in \u001b[0;36mContextualLexer.lex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    655\u001b[0m     last_token \u001b[38;5;241m=\u001b[39m lexer_state\u001b[38;5;241m.\u001b[39mlast_token  \u001b[38;5;66;03m# Save last_token. Calling root_lexer.next_token will change this to the wrong token\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_lexer\u001b[38;5;241m.\u001b[39mnext_token(lexer_state, parser_state)\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedToken(token, e\u001b[38;5;241m.\u001b[39mallowed, state\u001b[38;5;241m=\u001b[39mparser_state, token_history\u001b[38;5;241m=\u001b[39m[last_token], terminals_by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_lexer\u001b[38;5;241m.\u001b[39mterminals_by_name)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedCharacters:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mUnexpectedToken\u001b[0m: Unexpected token Token('ID', 'FUNCTION') at line 1, column 6.\nExpected one of: \n\t* INDEX\n\t* UDF\n\t* DATABASE\n\t* TABLE\nPrevious tokens: [Token('DROP', 'DROP')]\n"
     ]
    }
   ],
   "source": [
    "cursor.query(\"DROP FUNCTION IF EXISTS SiftFeatureExtractor;\").df()\n",
    "cursor.query(\"\"\"\n",
    "    CREATE FUNCTION SiftFeatureExtractor\n",
    "    IMPL '../evadb/functions/sift_feature_extractor.py'\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1101ec76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:21.277349Z",
     "iopub.status.busy": "2023-07-11T07:28:21.277097Z",
     "iopub.status.idle": "2023-07-11T07:28:21.280842Z",
     "shell.execute_reply": "2023-07-11T07:28:21.280177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep track of which image gets the most votes\n",
    "from collections import Counter\n",
    "vote = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a1e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T04:14:02.011536Z",
     "iopub.status.busy": "2023-05-10T04:14:02.011425Z",
     "iopub.status.idle": "2023-05-10T04:14:02.015115Z",
     "shell.execute_reply": "2023-05-10T04:14:02.014808Z"
    }
   },
   "source": [
    "## Image-level similarity search pipeline. \n",
    "This pipeline creates one vector per image. Next, we should breakdown steps how we build the index and search similar vectors using the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85e3fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:21.284447Z",
     "iopub.status.busy": "2023-07-11T07:28:21.284133Z",
     "iopub.status.idle": "2023-07-11T07:28:23.295499Z",
     "shell.execute_reply": "2023-07-11T07:28:23.294682Z"
    }
   },
   "outputs": [],
   "source": [
    "#1. Create index for the entire image\n",
    "cursor.query(\"\"\"DROP INDEX IF EXISTS reddit_sift_image_index\"\"\").df()\n",
    "cursor.query(\"\"\"\n",
    "    CREATE INDEX reddit_sift_image_index \n",
    "    ON reddit_dataset (SiftFeatureExtractor(data)) \n",
    "    USING FAISS\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54cfe6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:23.299139Z",
     "iopub.status.busy": "2023-07-11T07:28:23.298723Z",
     "iopub.status.idle": "2023-07-11T07:28:23.812001Z",
     "shell.execute_reply": "2023-07-11T07:28:23.810966Z"
    }
   },
   "outputs": [],
   "source": [
    "#2. Search similar vectors\n",
    "response = cursor.query(\"\"\"\n",
    "    SELECT name FROM reddit_dataset ORDER BY\n",
    "    Similarity(\n",
    "      SiftFeatureExtractor(Open('reddit-images/g1074_d4mxztt.jpg')),\n",
    "      SiftFeatureExtractor(data)\n",
    "    )\n",
    "    LIMIT 5\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68734588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:23.816749Z",
     "iopub.status.busy": "2023-07-11T07:28:23.816479Z",
     "iopub.status.idle": "2023-07-11T07:28:23.821672Z",
     "shell.execute_reply": "2023-07-11T07:28:23.820906Z"
    }
   },
   "outputs": [],
   "source": [
    "#3. Update votes\n",
    "for i in range(len(response)):\n",
    "    vote[response[\"reddit_dataset.name\"][i]] += 1\n",
    "print(vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a5d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T04:14:02.901910Z",
     "iopub.status.busy": "2023-05-10T04:14:02.901809Z",
     "iopub.status.idle": "2023-05-10T04:14:02.903869Z",
     "shell.execute_reply": "2023-05-10T04:14:02.903634Z"
    }
   },
   "source": [
    "## Object-level similarity search pipeline. \n",
    "This pipeline detects objects within images and generates vectors exclusively from the cropped objects. The index is then constructed using these vectors. To showcase the versatility of `evadb`, we leverage `Qdrant` vector store specifically for building this index. This demonstrates how seamlessly you can leverage different vector stores within evadb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1b7fe",
   "metadata": {},
   "source": [
    "### 1. Extract all the object using `Yolo` from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cefc8b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:23.825065Z",
     "iopub.status.busy": "2023-07-11T07:28:23.824820Z",
     "iopub.status.idle": "2023-07-11T07:28:28.436476Z",
     "shell.execute_reply": "2023-07-11T07:28:28.435614Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor.drop_table(\"reddit_object_table\").df()\n",
    "create_index_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS reddit_object_table\n",
    "    AS SELECT name, data, bboxes, labels FROM reddit_dataset\n",
    "    JOIN LATERAL UNNEST(Yolo(data)) AS Obj(labels, bboxes, scores);\"\"\"\n",
    "cursor.query(create_index_query).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356c435",
   "metadata": {},
   "source": [
    "### 2. Build an index on the feature vectors of the extracted objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0341be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:28.440928Z",
     "iopub.status.busy": "2023-07-11T07:28:28.440670Z",
     "iopub.status.idle": "2023-07-11T07:28:29.885240Z",
     "shell.execute_reply": "2023-07-11T07:28:29.884415Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor.query(\"\"\"\n",
    "    CREATE INDEX reddit_sift_object_index\n",
    "    ON reddit_object_table (SiftFeatureExtractor(Crop(data, bboxes)))\n",
    "    USING QDRANT\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17bfafc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:29.890024Z",
     "iopub.status.busy": "2023-07-11T07:28:29.889729Z",
     "iopub.status.idle": "2023-07-11T07:28:30.724204Z",
     "shell.execute_reply": "2023-07-11T07:28:30.723274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a cropped images (We are actively working on features to allow\n",
    "# us to not do this outside SQL)\n",
    "response = (\n",
    "    cursor.query(\n",
    "        \"LOAD IMAGE 'reddit-images/g1190_clna260.jpg' INTO reddit_search_image_dataset\"\n",
    "    )\n",
    "    .df()\n",
    ")\n",
    "print(response)\n",
    "response = (\n",
    "    cursor.query(\"SELECT Yolo(data).bboxes FROM reddit_search_image_dataset;\")\n",
    "    .df()\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import pathlib\n",
    "\n",
    "bboxes = response[\"yolo.bboxes\"][0]\n",
    "\n",
    "img = cv2.imread(\"reddit-images/g1190_clna260.jpg\")\n",
    "pathlib.Path(\"reddit-images/search-object/\").mkdir(parents=True, exist_ok=True)\n",
    "for i, bbox in enumerate(bboxes):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "    cropped_img = img[ymin:ymax, xmin:xmax]\n",
    "    cv2.imwrite(f\"reddit-images/search-object/search-{i}.jpg\", cropped_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e563b2",
   "metadata": {},
   "source": [
    "### 3. Retrieve using object-level similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f29c9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:30.729063Z",
     "iopub.status.busy": "2023-07-11T07:28:30.728825Z",
     "iopub.status.idle": "2023-07-11T07:28:35.086542Z",
     "shell.execute_reply": "2023-07-11T07:28:35.085747Z"
    }
   },
   "outputs": [],
   "source": [
    "#4. \n",
    "import os\n",
    "\n",
    "for path in os.listdir(\"reddit-images/search-object/\"):\n",
    "    path = \"reddit-images/search-object/\" + path\n",
    "    query = f\"\"\"SELECT name FROM reddit_object_table ORDER BY\n",
    "                        Similarity(\n",
    "                          SiftFeatureExtractor(Open('{path}')),\n",
    "                          SiftFeatureExtractor(data)\n",
    "                        )\n",
    "                        LIMIT 1\"\"\"\n",
    "    response = cursor.query(query).df()\n",
    "    for i in range(len(response)):\n",
    "        vote[response[\"reddit_object_table.name\"][i]] += 0.5\n",
    "\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d4970",
   "metadata": {},
   "source": [
    "## Combine the scores from image level and object level similarity to show similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc67393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:28:35.091090Z",
     "iopub.status.busy": "2023-07-11T07:28:35.090846Z",
     "iopub.status.idle": "2023-07-11T07:28:36.224117Z",
     "shell.execute_reply": "2023-07-11T07:28:36.223321Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display top images\n",
    "vote_list = list(reversed(sorted([(path, count) for path, count in vote.items()], key=lambda x: x[1])))\n",
    "img_list = [path for path, _ in vote_list]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=[18,10])\n",
    "ax[0].imshow(cv2.imread(\"reddit-images/g1190_clna260.jpg\"))\n",
    "ax[0].set_title(\"Search\")\n",
    "\n",
    "for i in range(5):\n",
    "    axi = ax[i + 1]\n",
    "    img = cv2.imread(img_list[i])\n",
    "    axi.imshow(img)\n",
    "    axi.set_title(f\"Top-{i + 1}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

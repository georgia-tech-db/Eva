{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a4df227",
   "metadata": {},
   "source": [
    "# Similarity search for motif mining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d29b79c",
   "metadata": {},
   "source": [
    "In this tutorial, we demonstrate how to utilize the similarity functionality to discover images with similar motifs from a collection of Reddit images. We employ the classic `SIFT` feature to identify images with a strikingly similar appearance (image-level pipeline).\n",
    "\n",
    "Additionally, we extend the pipeline by incorporating an object detection model, `YOLO`, in combination with the SIFT feature. This enables us to identify objects within the images that exhibit a similar appearance (object-level similarity).\n",
    "\n",
    "To illustrate the seamless integration of different vector stores, we leverage the power of multiple vector stores, namely `FAISS` and `QDRANT`, within evadb. This demonstrates the ease with which you can utilize diverse vector stores to construct indexes, enhancing your similarity search experience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6df3057",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/georgia-tech-db/eva/blob/master/tutorials/11-similarity-search-for-motif-mining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run on Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/georgia-tech-db/eva/blob/master/tutorials/11-similarity-search-for-motif-mining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/georgia-tech-db/eva/raw/master/tutorials/11-similarity-search-for-motif-mining.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /> Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50b1674e",
   "metadata": {},
   "source": [
    "### Start EVA server\n",
    "\n",
    "We are reusing the start server notebook for launching the EVA server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece6b332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:23.961486Z",
     "iopub.status.busy": "2023-05-30T08:09:23.961041Z",
     "iopub.status.idle": "2023-05-30T08:09:43.987553Z",
     "shell.execute_reply": "2023-05-30T08:09:43.985994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘00-start-eva-server.ipynb’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping EVA Server ...\n",
      "Starting EVA Server ...\n",
      "nohup eva_server > eva.log 2>&1 &\n"
     ]
    }
   ],
   "source": [
    "!wget -nc \"https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/00-start-eva-server.ipynb\"\n",
    "%run 00-start-eva-server.ipynb\n",
    "cursor = connect_to_server()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc9f4144",
   "metadata": {},
   "source": [
    "### Download reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527ec1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:43.992847Z",
     "iopub.status.busy": "2023-05-30T08:09:43.992399Z",
     "iopub.status.idle": "2023-05-30T08:09:44.276667Z",
     "shell.execute_reply": "2023-05-30T08:09:44.274858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘reddit-images.zip’ already there; not retrieving.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  reddit-images.zip\r\n",
      "warning:  stripped absolute path spec from /\r\n",
      "mapname:  conversion of  failed\r\n",
      " extracting: reddit-images/g348_d7jgzgf.jpg  \r\n",
      " extracting: reddit-images/g348_d7jphyc.jpg  \r\n",
      " extracting: reddit-images/g348_d7ju7dq.jpg  \r\n",
      " extracting: reddit-images/g348_d7jhhs3.jpg  \r\n",
      " extracting: reddit-images/g1074_d4n1lmn.jpg  \r\n",
      " extracting: reddit-images/g1074_d4mxztt.jpg  \r\n",
      " extracting: reddit-images/g1074_d4n60oy.jpg  \r\n",
      " extracting: reddit-images/g1074_d4n6fgs.jpg  \r\n",
      " extracting: reddit-images/g1190_cln9xzr.jpg  \r\n",
      " extracting: reddit-images/g1190_cln97xm.jpg  \r\n",
      " extracting: reddit-images/g1190_clna260.jpg  \r\n",
      " extracting: reddit-images/g1190_clna2x2.jpg  \r\n",
      " extracting: reddit-images/g1190_clna91w.jpg  \r\n",
      " extracting: reddit-images/g1190_clnad42.jpg  \r\n",
      " extracting: reddit-images/g1190_clnajd7.jpg  \r\n",
      " extracting: reddit-images/g1190_clnapoy.jpg  \r\n",
      " extracting: reddit-images/g1190_clnarjl.jpg  \r\n",
      " extracting: reddit-images/g1190_clnavnu.jpg  \r\n",
      " extracting: reddit-images/g1190_clnbalu.jpg  \r\n",
      " extracting: reddit-images/g1190_clnbf07.jpg  \r\n",
      " extracting: reddit-images/g1190_clnc4uy.jpg  \r\n",
      " extracting: reddit-images/g1190_clncot0.jpg  \r\n",
      " extracting: reddit-images/g1190_clndsnu.jpg  \r\n",
      " extracting: reddit-images/g1190_clnce4b.jpg  \r\n",
      " extracting: reddit-images/g1209_ct65pvl.jpg  \r\n",
      " extracting: reddit-images/g1209_ct66erw.jpg  \r\n",
      " extracting: reddit-images/g1209_ct67oqk.jpg  \r\n",
      " extracting: reddit-images/g1209_ct6a0g5.jpg  \r\n",
      " extracting: reddit-images/g1209_ct6bf1n.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3o1h6.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3om3h.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3qysz.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3r4gw.jpg  \r\n",
      " extracting: reddit-images/g1418_cj3z7jw.jpg  \r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget -nc https://www.dropbox.com/scl/fo/fcj6ojmii0gw92zg3jb2s/h\\?dl\\=1\\&rlkey\\=j3kj1ox4yn5fhonw06v0pn7r9 -O reddit-images.zip\n",
    "!unzip -o reddit-images.zip -d reddit-images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d8c9917",
   "metadata": {},
   "source": [
    "### Load all images into evadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9bca7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:44.282956Z",
     "iopub.status.busy": "2023-05-30T08:09:44.282622Z",
     "iopub.status.idle": "2023-05-30T08:09:44.789817Z",
     "shell.execute_reply": "2023-05-30T08:09:44.789061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number of loaded IMAGE: 34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "0  Number of loaded IMAGE: 34"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = cursor.execute(\"DROP TABLE IF EXISTS reddit_dataset;\").fetch_all().as_df()\n",
    "cursor.execute(\n",
    "    \"LOAD IMAGE 'reddit-images/*.jpg' INTO reddit_dataset;\"\n",
    ").fetch_all().as_df()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6743684c",
   "metadata": {},
   "source": [
    "### Register a SIFT FeatureExtractor \n",
    "It uses `kornia` library to extract sift features for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "182dfe81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:44.792960Z",
     "iopub.status.busy": "2023-05-30T08:09:44.792710Z",
     "iopub.status.idle": "2023-05-30T08:09:47.726108Z",
     "shell.execute_reply": "2023-05-30T08:09:47.724201Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kornia --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49496e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:47.731422Z",
     "iopub.status.busy": "2023-05-30T08:09:47.731096Z",
     "iopub.status.idle": "2023-05-30T08:09:47.756669Z",
     "shell.execute_reply": "2023-05-30T08:09:47.755911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UDF SiftFeatureExtractor already exists, nothi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  UDF SiftFeatureExtractor already exists, nothi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"CREATE UDF IF NOT EXISTS SiftFeatureExtractor\n",
    "                    IMPL  '../eva/udfs/sift_feature_extractor.py'\"\"\").fetch_all().as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1101ec76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:47.760972Z",
     "iopub.status.busy": "2023-05-30T08:09:47.760655Z",
     "iopub.status.idle": "2023-05-30T08:09:47.764941Z",
     "shell.execute_reply": "2023-05-30T08:09:47.764139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep track of which image gets the most votes\n",
    "from collections import Counter\n",
    "vote = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a1e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T04:14:02.011536Z",
     "iopub.status.busy": "2023-05-10T04:14:02.011425Z",
     "iopub.status.idle": "2023-05-10T04:14:02.015115Z",
     "shell.execute_reply": "2023-05-10T04:14:02.014808Z"
    }
   },
   "source": [
    "## Image-level similarity search pipeline. \n",
    "This pipeline creates one vector per image. Next, we should breakdown steps how we build the index and search similar vectors using the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85e3fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:47.768002Z",
     "iopub.status.busy": "2023-05-30T08:09:47.767735Z",
     "iopub.status.idle": "2023-05-30T08:09:50.753932Z",
     "shell.execute_reply": "2023-05-30T08:09:50.753182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Index reddit_sift_image_index successfully add...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Index reddit_sift_image_index successfully add..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Create index for the entire image\n",
    "cursor.execute(\"\"\"CREATE INDEX reddit_sift_image_index \n",
    "                    ON reddit_dataset (SiftFeatureExtractor(data)) \n",
    "                    USING FAISS\"\"\").fetch_all().as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54cfe6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:50.757496Z",
     "iopub.status.busy": "2023-05-30T08:09:50.757238Z",
     "iopub.status.idle": "2023-05-30T08:09:51.452804Z",
     "shell.execute_reply": "2023-05-30T08:09:51.451345Z"
    }
   },
   "outputs": [],
   "source": [
    "#2. Search similar vectors\n",
    "response = cursor.execute(\"\"\"SELECT name FROM reddit_dataset ORDER BY\n",
    "                    Similarity(\n",
    "                      SiftFeatureExtractor(Open('reddit-images/g1190_clna260.jpg')),\n",
    "                      SiftFeatureExtractor(data)\n",
    "                    )\n",
    "                    LIMIT 5\"\"\").fetch_all().as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68734588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:51.457969Z",
     "iopub.status.busy": "2023-05-30T08:09:51.457567Z",
     "iopub.status.idle": "2023-05-30T08:09:51.462980Z",
     "shell.execute_reply": "2023-05-30T08:09:51.462360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'reddit-images/g1190_clna260.jpg': 1, 'reddit-images/g1190_clndsnu.jpg': 1, 'reddit-images/g1190_clna91w.jpg': 1, 'reddit-images/g1190_clnc4uy.jpg': 1, 'reddit-images/g1190_cln97xm.jpg': 1})\n"
     ]
    }
   ],
   "source": [
    "#3. Update votes\n",
    "for i in range(len(response)):\n",
    "    vote[response[\"reddit_dataset.name\"][i]] += 1\n",
    "print(vote)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a19a5d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T04:14:02.901910Z",
     "iopub.status.busy": "2023-05-10T04:14:02.901809Z",
     "iopub.status.idle": "2023-05-10T04:14:02.903869Z",
     "shell.execute_reply": "2023-05-10T04:14:02.903634Z"
    }
   },
   "source": [
    "## Object-level similarity search pipeline. \n",
    "This pipeline detects objects within images and generates vectors exclusively from the cropped objects. The index is then constructed using these vectors. To showcase the versatility of `evadb`, we leverage `Qdrant` vector store specifically for building this index. This demonstrates how seamlessly you can leverage different vector stores within evadb."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bce1b7fe",
   "metadata": {},
   "source": [
    "### 1. Extract all the object using `Yolo` from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cefc8b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T08:09:51.465556Z",
     "iopub.status.busy": "2023-05-30T08:09:51.465271Z",
     "iopub.status.idle": "2023-05-30T08:09:53.761183Z",
     "shell.execute_reply": "2023-05-30T08:09:53.760155Z"
    }
   },
   "outputs": [
    {
     "ename": "ExecutorError",
     "evalue": "Table: reddit_object_table already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExecutorError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m create_index_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    CREATE MATERIALIZED VIEW reddit_object_table (name, data, bboxes,labels)\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m      AS SELECT name, data, bboxes, labels FROM reddit_dataset\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m      JOIN LATERAL UNNEST(Yolo(data)) AS Obj(labels, bboxes, scores)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_index_query\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zxu330/eva/eva/models/server/response.py:50\u001b[0m, in \u001b[0;36mResponse.as_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_df\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutorError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutorError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mExecutorError\u001b[0m: Table: reddit_object_table already exists"
     ]
    }
   ],
   "source": [
    "create_index_query = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW reddit_object_table (name, data, bboxes,labels)\n",
    "      AS SELECT name, data, bboxes, labels FROM reddit_dataset\n",
    "      JOIN LATERAL UNNEST(Yolo(data)) AS Obj(labels, bboxes, scores)\"\"\"\n",
    "cursor.execute(create_index_query).fetch_all().as_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b356c435",
   "metadata": {},
   "source": [
    "### 2. Build an index on the feature vectors of the extracted objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc0341be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:12:43.770277Z",
     "iopub.status.busy": "2023-05-29T06:12:43.770003Z",
     "iopub.status.idle": "2023-05-29T06:12:45.274404Z",
     "shell.execute_reply": "2023-05-29T06:12:45.273612Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"CREATE INDEX reddit_sift_object_index\n",
    "                    ON reddit_object_table (SiftFeatureExtractor(Crop(data, bboxes)))\n",
    "                    USING QDRANT\"\"\").fetch_all().as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17bfafc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:12:45.277879Z",
     "iopub.status.busy": "2023-05-29T06:12:45.277655Z",
     "iopub.status.idle": "2023-05-29T06:12:46.762815Z",
     "shell.execute_reply": "2023-05-29T06:12:46.761529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a cropped images (We are actively working on features to allow\n",
    "# us to not do this outside SQL)\n",
    "response = (\n",
    "    cursor.execute(\n",
    "        \"LOAD IMAGE 'reddit-images/g1190_clna260.jpg' INTO reddit_search_image_dataset\"\n",
    "    )\n",
    "    .fetch_all()\n",
    "    .as_df()\n",
    ")\n",
    "print(response)\n",
    "response = (\n",
    "    cursor.execute(\"SELECT Yolo(data).bboxes FROM reddit_search_image_dataset;\")\n",
    "    .fetch_all()\n",
    "    .as_df()\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import pathlib\n",
    "\n",
    "bboxes = response[\"yolo.bboxes\"][0]\n",
    "\n",
    "img = cv2.imread(\"reddit-images/g1190_clna260.jpg\")\n",
    "pathlib.Path(\"reddit-images/search-object/\").mkdir(parents=True, exist_ok=True)\n",
    "for i, bbox in enumerate(bboxes):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "    cropped_img = img[ymin:ymax, xmin:xmax]\n",
    "    cv2.imwrite(f\"reddit-images/search-object/search-{i}.jpg\", cropped_img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40e563b2",
   "metadata": {},
   "source": [
    "### 3. Retrieve using object-level similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f29c9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:12:46.767621Z",
     "iopub.status.busy": "2023-05-29T06:12:46.767312Z",
     "iopub.status.idle": "2023-05-29T06:12:50.665330Z",
     "shell.execute_reply": "2023-05-29T06:12:50.664130Z"
    }
   },
   "outputs": [],
   "source": [
    "#4. \n",
    "import os\n",
    "\n",
    "for path in os.listdir(\"reddit-images/search-object/\"):\n",
    "    path = \"reddit-images/search-object/\" + path\n",
    "    cursor.execute(f\"\"\"SELECT name FROM reddit_object_table ORDER BY\n",
    "                        Similarity(\n",
    "                          SiftFeatureExtractor(Open('{path}')),\n",
    "                          SiftFeatureExtractor(data)\n",
    "                        )\n",
    "                        LIMIT 1\"\"\")\n",
    "    response = cursor.fetch_all().as_df()\n",
    "    for i in range(len(response)):\n",
    "        vote[response[\"reddit_object_table.name\"][i]] += 0.5\n",
    "\n",
    "    print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "096d4970",
   "metadata": {},
   "source": [
    "## Combine the scores from image level and object level similarity to show similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc67393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:12:50.670869Z",
     "iopub.status.busy": "2023-05-29T06:12:50.670284Z",
     "iopub.status.idle": "2023-05-29T06:12:52.312895Z",
     "shell.execute_reply": "2023-05-29T06:12:52.311934Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display top images\n",
    "vote_list = list(reversed(sorted([(path, count) for path, count in vote.items()], key=lambda x: x[1])))\n",
    "img_list = [path for path, _ in vote_list]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=[18,10])\n",
    "ax[0].imshow(cv2.imread(\"reddit-images/g1190_clna260.jpg\"))\n",
    "ax[0].set_title(\"Search\")\n",
    "\n",
    "for i in range(5):\n",
    "    axi = ax[i + 1]\n",
    "    img = cv2.imread(img_list[i])\n",
    "    axi.imshow(img)\n",
    "    axi.set_title(f\"Top-{i + 1}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

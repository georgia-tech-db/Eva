{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import time\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "#from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva/others'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva/data/mnist'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = os.path.abspath('../')\n",
    "data_dir = os.path.join(root_dir, 'data', 'mnist')\n",
    "root_dir\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6b51438c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = 7777 # You may select anything up to 60,000\n",
    "print(train_y[image_index]) # The label is 8\n",
    "plt.imshow(train_x[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, 784).astype('float32')\n",
    "test_x = test_x.reshape(-1, 784).astype('float32')\n",
    "train_x /= 255.0\n",
    "test_x /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time it took to train k_means is  613.3203248977661  seconds\n"
     ]
    }
   ],
   "source": [
    "# Simple KMeans\n",
    "km = KMeans(n_jobs=-1, n_clusters=10, n_init=20)\n",
    "start_time = time.time()\n",
    "km.fit(train_x)\n",
    "print(\"Total time it took to train k_means is \", time.time() - start_time, \" seconds\")\n",
    "pred = km.predict(test_x)\n",
    "kmeans_score = normalized_mutual_info_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 50)                100050    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2000)              102000    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 784)               392784    \n",
      "=================================================================\n",
      "Total params: 3,490,834\n",
      "Trainable params: 3,490,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Autoencoder network\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(500, activation='relu')(input_img)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "\n",
    "#Value I want to play around with is thisi signmoid\n",
    "encoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(2000, activation='relu')(encoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(784)(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0731 - val_loss: 0.0654\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0632 - val_loss: 0.0597\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0576 - val_loss: 0.0537\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0485 - val_loss: 0.0441\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0405 - val_loss: 0.0367\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0344 - val_loss: 0.0324\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0308 - val_loss: 0.0287\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0277 - val_loss: 0.0263\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0254 - val_loss: 0.0240\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0238 - val_loss: 0.0224\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0222 - val_loss: 0.0211\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0208 - val_loss: 0.0195\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0197 - val_loss: 0.0190\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0185 - val_loss: 0.0176\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0177 - val_loss: 0.0168\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0169 - val_loss: 0.0163\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0161 - val_loss: 0.0157\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0154 - val_loss: 0.0150\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0151 - val_loss: 0.0146\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0145 - val_loss: 0.0143\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0137 - val_loss: 0.0132\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0132 - val_loss: 0.0134\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0126 - val_loss: 0.0127\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0124 - val_loss: 0.0122\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0120 - val_loss: 0.0118\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0118 - val_loss: 0.0113\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0099\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0091 - val_loss: 0.0092\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0091 - val_loss: 0.0093\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0089 - val_loss: 0.0090\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0087 - val_loss: 0.0088\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0086 - val_loss: 0.0087\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0084 - val_loss: 0.0085\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0083 - val_loss: 0.0085\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0082 - val_loss: 0.0084\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0082 - val_loss: 0.0083\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0079 - val_loss: 0.0082\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0080 - val_loss: 0.0082\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0077 - val_loss: 0.0080\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0078 - val_loss: 0.0080\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0077 - val_loss: 0.0083\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0076 - val_loss: 0.0079\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0076 - val_loss: 0.0078\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0075 - val_loss: 0.0077\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0075 - val_loss: 0.0077\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0075 - val_loss: 0.0080\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0074 - val_loss: 0.0076\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0073 - val_loss: 0.0077\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0073 - val_loss: 0.0077\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0072 - val_loss: 0.0076\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0072 - val_loss: 0.0074\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0071 - val_loss: 0.0076\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0069 - val_loss: 0.0074\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0063 - val_loss: 0.0068\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0060 - val_loss: 0.0066\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0060 - val_loss: 0.0064\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0059 - val_loss: 0.0064\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0060 - val_loss: 0.0063\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0063\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0062\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0067\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0062\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0063\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0063\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0058 - val_loss: 0.0063\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0062\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0059 - val_loss: 0.0063\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0058 - val_loss: 0.0062\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0062\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0062\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0058 - val_loss: 0.0062\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0058 - val_loss: 0.0061\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0063\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0062\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0065\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0056 - val_loss: 0.0063\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0057 - val_loss: 0.0060\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0056 - val_loss: 0.0061\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0056 - val_loss: 0.0061\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0056 - val_loss: 0.0059\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0060\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0054 - val_loss: 0.0059\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0060\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0060\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0054 - val_loss: 0.0059\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0055 - val_loss: 0.0058\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0054 - val_loss: 0.0059\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0054 - val_loss: 0.0064\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0053 - val_loss: 0.0057\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0053 - val_loss: 0.0060\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0053 - val_loss: 0.0060\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0054 - val_loss: 0.0057\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0053 - val_loss: 0.0058\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0053 - val_loss: 0.0059\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0053 - val_loss: 0.0059\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0053 - val_loss: 0.0058\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0057\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0053 - val_loss: 0.0060\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0057\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0057\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0057\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0052 - val_loss: 0.0056\n",
      "Total time it took to train autoencoder is  338.09144020080566  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_history = autoencoder.fit(train_x, train_x, epochs=200, batch_size=2048, validation_data=(test_x, test_x))\n",
    "print(\"Total time it took to train autoencoder is \", time.time() - start_time, \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| nodes | epoch | loss | k-means accuracy|\n",
    "| --- | --- | --- | --- |\n",
    "| 5 | 200 | 0.022 | 0.7  |\n",
    "| 10 | 500 | 0.01 | 0.8  |\n",
    "| 20 | 200 | 0.088 | 0.66   |\n",
    "| 50 | 200 | 0.0056 | 0.57 |\n",
    "| 100 | 200 | 0.05 |  0.5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auto_train = encoder.predict(train_x)\n",
    "pred_auto = encoder.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24276054, 0.31561896, 0.6970326 , 0.60552293, 0.4603314 ,\n",
       "       0.2988927 , 0.42137945, 0.36478987, 0.159174  , 0.7464836 ,\n",
       "       0.4647177 , 0.46423352, 0.42450443, 0.5328621 , 0.28814837,\n",
       "       0.46265915, 0.6122539 , 0.3371671 , 0.36244312, 0.15607393,\n",
       "       0.5333175 , 0.4629024 , 0.20623149, 0.20255852, 0.5381532 ,\n",
       "       0.21110724, 0.27205837, 0.23290189, 0.5586298 , 0.7902839 ,\n",
       "       0.4829538 , 0.5424529 , 0.29667726, 0.48150453, 0.33089536,\n",
       "       0.4275643 , 0.43934926, 0.5620069 , 0.3493944 , 0.7728975 ,\n",
       "       0.870998  , 0.39995232, 0.16171396, 0.44085842, 0.58879215,\n",
       "       0.3978635 , 0.13912138, 0.6388631 , 0.2563932 , 0.28466693],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_auto_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_auto = KMeans(n_jobs=-1, n_clusters=10, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time it took to train k_means is  37.32961106300354  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "km_auto.fit(pred_auto_train)\n",
    "print(\"Total time it took to train k_means is \", time.time() - start_time, \" seconds\")\n",
    "\n",
    "pred = km_auto.predict(pred_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5703520881792029"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_score = normalized_mutual_info_score(test_y, pred)\n",
    "auto_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
    "\n",
    "Original Author:\n",
    "    Xifeng Guo. 2017.1.30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (metrics.accuracy_score(self.y, y_pred), metrics.normalized_mutual_info_score(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time.time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: ', time.time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time.time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.accuracy_score(y, y_pred), 5)\n",
    "                    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "                    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    \n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 2048\n",
    "maxiter = 2e4\n",
    "tol = 0.0001\n",
    "save_dir = 'analysis_results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "update_interval = 100\n",
    "pretrain_epochs = 200\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                       distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "#pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "\n",
    "\n",
    "# prepare the DEC model\n",
    "dec = DEC(dims=[train_x.shape[-1], 500, 500, 2000, 10], n_clusters=10, init=init)\n",
    "\n",
    "dec.pretrain(x=train_x, y=train_y, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 1,665,110\n",
      "Trainable params: 1,665,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 200\n",
      "Save interval 146.484375\n",
      "Initializing cluster centers with k-means.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.05088, nmi = 0.77332, ari = 0.72686  ; loss= 0\n",
      "saving model to: results/DEC_model_0.h5\n",
      "Iter 200: acc = 0.05050, nmi = 0.77894, ari = 0.73977  ; loss= 0\n",
      "Iter 400: acc = 0.05045, nmi = 0.79176, ari = 0.75790  ; loss= 0\n",
      "Iter 600: acc = 0.04940, nmi = 0.80387, ari = 0.77173  ; loss= 0\n",
      "Iter 800: acc = 0.04915, nmi = 0.81126, ari = 0.77959  ; loss= 0\n",
      "Iter 1000: acc = 0.04907, nmi = 0.81544, ari = 0.78393  ; loss= 0\n",
      "Iter 1200: acc = 0.04887, nmi = 0.81856, ari = 0.78698  ; loss= 0\n",
      "Iter 1400: acc = 0.04897, nmi = 0.82119, ari = 0.78957  ; loss= 0\n",
      "Iter 1600: acc = 0.04893, nmi = 0.82321, ari = 0.79143  ; loss= 0\n",
      "Iter 1800: acc = 0.04865, nmi = 0.82513, ari = 0.79332  ; loss= 0\n",
      "Iter 2000: acc = 0.04868, nmi = 0.82601, ari = 0.79421  ; loss= 0\n",
      "Iter 2200: acc = 0.04862, nmi = 0.82645, ari = 0.79450  ; loss= 0\n",
      "Iter 2400: acc = 0.04857, nmi = 0.82719, ari = 0.79518  ; loss= 0\n",
      "Iter 2600: acc = 0.04857, nmi = 0.82789, ari = 0.79591  ; loss= 0\n",
      "Iter 2800: acc = 0.04865, nmi = 0.82805, ari = 0.79581  ; loss= 0\n",
      "Iter 3000: acc = 0.04860, nmi = 0.82827, ari = 0.79602  ; loss= 0\n",
      "Iter 3200: acc = 0.04872, nmi = 0.82841, ari = 0.79629  ; loss= 0\n",
      "Iter 3400: acc = 0.04878, nmi = 0.82864, ari = 0.79620  ; loss= 0\n",
      "Iter 3600: acc = 0.04885, nmi = 0.82860, ari = 0.79612  ; loss= 0\n",
      "Iter 3800: acc = 0.04887, nmi = 0.82872, ari = 0.79633  ; loss= 0\n",
      "Iter 4000: acc = 0.04888, nmi = 0.82899, ari = 0.79633  ; loss= 0\n",
      "Iter 4200: acc = 0.04888, nmi = 0.82891, ari = 0.79625  ; loss= 0\n",
      "Iter 4400: acc = 0.04897, nmi = 0.82885, ari = 0.79626  ; loss= 0\n",
      "Iter 4600: acc = 0.04897, nmi = 0.82919, ari = 0.79641  ; loss= 0\n",
      "Iter 4800: acc = 0.04888, nmi = 0.82933, ari = 0.79656  ; loss= 0\n",
      "Iter 5000: acc = 0.04882, nmi = 0.82950, ari = 0.79674  ; loss= 0\n",
      "Iter 5200: acc = 0.04888, nmi = 0.82934, ari = 0.79652  ; loss= 0\n",
      "Iter 5400: acc = 0.04888, nmi = 0.82938, ari = 0.79660  ; loss= 0\n",
      "Iter 5600: acc = 0.04888, nmi = 0.82939, ari = 0.79665  ; loss= 0\n",
      "Iter 5800: acc = 0.04893, nmi = 0.82951, ari = 0.79673  ; loss= 0\n",
      "Iter 6000: acc = 0.04897, nmi = 0.82957, ari = 0.79675  ; loss= 0\n",
      "Iter 6200: acc = 0.04892, nmi = 0.82969, ari = 0.79689  ; loss= 0\n",
      "Iter 6400: acc = 0.04897, nmi = 0.82967, ari = 0.79673  ; loss= 0\n",
      "Iter 6600: acc = 0.04893, nmi = 0.82976, ari = 0.79684  ; loss= 0\n",
      "Iter 6800: acc = 0.04895, nmi = 0.82972, ari = 0.79680  ; loss= 0\n",
      "delta_label  8.333333333333333e-05 < tol  0.0001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: results/DEC_model_final.h5\n"
     ]
    }
   ],
   "source": [
    "y_pred = dec.fit(train_x, y=train_y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = dec.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351783283745353"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_score = normalized_mutual_info_score(test_y, pred_val)\n",
    "dec_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to plot the accuracies and save the results\n",
    "# kmeans_score = 0.49\n",
    "# auto_score = 0.8\n",
    "# dec_score = 0.835\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.bar([0,1,2], [kmeans_score, auto_score, dec_score])\n",
    "plt.xticks([0,1,2], ['kmeans', 'auto', 'DEC'], rotation=30)\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores for K-means, Autoencoder, DEC')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "savefig('scores_for_prototype.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp36",
   "language": "python",
   "name": "pp36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

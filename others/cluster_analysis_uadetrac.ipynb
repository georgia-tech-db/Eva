{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this file, we will first train an autoencoder with UA-detrac, cluster it and use the labels to train the DEC network\n",
    "First we will have to do a manual examination of the result from autoencoder / cluster to see if it is appropriate to use it as a benchmark - No\n",
    "If so, we will then compare the results with clustering using a distance approach with k-means and DEC algorithm\n",
    "There is also a clustering method called hypergraph.. see if we have time for this\n",
    "There are other clustering methods that don't require a predefined number of clusters...\n",
    "However, for now, we will go about using k-means where the number of cluster is frame_num / 20\n",
    "\"\"\"\n",
    "\n",
    "%pylab inline\n",
    "import os\n",
    "import keras\n",
    "import metrics\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva/data/ua_detrac/small-data/MVI_20011'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "home_dir = os.path.abspath('../')\n",
    "data_dir = os.path.join(home_dir, 'data', 'ua_detrac', 'small-data', 'MVI_20011')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_nn(image_dir, downsize_rate = 1):\n",
    "    \"\"\"\n",
    "    Loading images in a non normalized form\n",
    "    :param image_dir:\n",
    "    :param downsize_rate:\n",
    "    :param grayscale:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image_height = 540\n",
    "    image_width = 960\n",
    "    file_names = []\n",
    "    for root, subdirs, files in os.walk(image_dir):\n",
    "        files.sort()\n",
    "        for file in files:\n",
    "            file_names.append(os.path.join(root, file))\n",
    "\n",
    "    \n",
    "    img_table = np.ndarray(shape=(len(file_names), int(image_height / downsize_rate), int(image_width / downsize_rate)))\n",
    "    \n",
    "    for i in range(len(file_names)):\n",
    "        file_name = file_names[i]\n",
    "        img = cv2.imread(file_name,0)\n",
    "        img = cv2.resize(img, (image_width // downsize_rate, image_height // downsize_rate))\n",
    "        img_table[i] = img\n",
    "\n",
    "    return img_table\n",
    "\n",
    "image_table = load_images_nn(data_dir, downsize_rate = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training and test data\n",
    "\n",
    "n_samples = image_table.shape[0]\n",
    "train_x = image_table[:int(n_samples * 0.8)]\n",
    "test_x = image_table[int(n_samples * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5f992d75f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "image_index = random.randint(0,100) # You may select anything up to 60,000\n",
    "print(train_x[image_index].shape)\n",
    "\n",
    "plt.imshow(train_x[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, width, height = image_table.shape\n",
    "train_x = train_x.reshape(-1, width*height).astype('float32')\n",
    "test_x = test_x.reshape(-1, width*height).astype('float32')\n",
    "\n",
    "# Must perform normalization for the autoencoder to perform well\n",
    "train_x /= 255.0\n",
    "test_x /= 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEC Implementation\n",
    "\n",
    "\n",
    "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
    "\n",
    "\n",
    "Original Author:\n",
    "    Xifeng Guo. 2017.1.30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (metrics.acc(self.y, y_pred), metrics.nmi(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time.time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: ', time.time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time.time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "                    nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "                    ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/500\n",
      "531/531 [==============================] - 2s 4ms/step - loss: 0.2446\n",
      "Epoch 2/500\n",
      "531/531 [==============================] - 0s 164us/step - loss: 0.2436\n",
      "Epoch 3/500\n",
      "531/531 [==============================] - 0s 150us/step - loss: 0.2363\n",
      "Epoch 4/500\n",
      "531/531 [==============================] - 0s 154us/step - loss: 0.1882\n",
      "Epoch 5/500\n",
      "531/531 [==============================] - 0s 72us/step - loss: 0.6389\n",
      "Epoch 6/500\n",
      "531/531 [==============================] - 0s 128us/step - loss: 0.1202\n",
      "Epoch 7/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.1816\n",
      "Epoch 8/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.2154\n",
      "Epoch 9/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.2263\n",
      "Epoch 10/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.2268\n",
      "Epoch 11/500\n",
      "531/531 [==============================] - 0s 75us/step - loss: 0.2189\n",
      "Epoch 12/500\n",
      "531/531 [==============================] - 0s 105us/step - loss: 0.1972\n",
      "Epoch 13/500\n",
      "531/531 [==============================] - 0s 102us/step - loss: 0.1523\n",
      "Epoch 14/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0956\n",
      "Epoch 15/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.2014\n",
      "Epoch 16/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0744\n",
      "Epoch 17/500\n",
      "531/531 [==============================] - 0s 102us/step - loss: 0.0651\n",
      "Epoch 18/500\n",
      "531/531 [==============================] - 0s 92us/step - loss: 0.0834\n",
      "Epoch 19/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0889\n",
      "Epoch 20/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0779\n",
      "Epoch 21/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0534\n",
      "Epoch 22/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0287\n",
      "Epoch 23/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0384\n",
      "Epoch 24/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0580\n",
      "Epoch 25/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0322\n",
      "Epoch 26/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0197\n",
      "Epoch 27/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0272\n",
      "Epoch 28/500\n",
      "531/531 [==============================] - 0s 91us/step - loss: 0.0343\n",
      "Epoch 29/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0332\n",
      "Epoch 30/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0254\n",
      "Epoch 31/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0182\n",
      "Epoch 32/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0208\n",
      "Epoch 33/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0280\n",
      "Epoch 34/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0241\n",
      "Epoch 35/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0173\n",
      "Epoch 36/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0177\n",
      "Epoch 37/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0211\n",
      "Epoch 38/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0220\n",
      "Epoch 39/500\n",
      "531/531 [==============================] - 0s 84us/step - loss: 0.0192\n",
      "Epoch 40/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0159\n",
      "Epoch 41/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0159\n",
      "Epoch 42/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0186\n",
      "Epoch 43/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0184\n",
      "Epoch 44/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0156\n",
      "Epoch 45/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0149\n",
      "Epoch 46/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0162\n",
      "Epoch 47/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0170\n",
      "Epoch 48/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0160\n",
      "Epoch 49/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0146\n",
      "Epoch 50/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0146\n",
      "Epoch 51/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0157\n",
      "Epoch 52/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0157\n",
      "Epoch 53/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0146\n",
      "Epoch 54/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0143\n",
      "Epoch 55/500\n",
      "531/531 [==============================] - 0s 111us/step - loss: 0.0150\n",
      "Epoch 56/500\n",
      "531/531 [==============================] - 0s 80us/step - loss: 0.0153\n",
      "Epoch 57/500\n",
      "531/531 [==============================] - 0s 92us/step - loss: 0.0148\n",
      "Epoch 58/500\n",
      "531/531 [==============================] - 0s 92us/step - loss: 0.0142\n",
      "Epoch 59/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0144\n",
      "Epoch 60/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0148\n",
      "Epoch 61/500\n",
      "531/531 [==============================] - 0s 160us/step - loss: 0.0146\n",
      "Epoch 62/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0142\n",
      "Epoch 63/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0142\n",
      "Epoch 64/500\n",
      "531/531 [==============================] - 0s 160us/step - loss: 0.0145\n",
      "Epoch 65/500\n",
      "531/531 [==============================] - 0s 148us/step - loss: 0.0145\n",
      "Epoch 66/500\n",
      "531/531 [==============================] - 0s 162us/step - loss: 0.0142\n",
      "Epoch 67/500\n",
      "531/531 [==============================] - 0s 108us/step - loss: 0.0141\n",
      "Epoch 68/500\n",
      "531/531 [==============================] - 0s 98us/step - loss: 0.0143\n",
      "Epoch 69/500\n",
      "531/531 [==============================] - 0s 112us/step - loss: 0.0143\n",
      "Epoch 70/500\n",
      "531/531 [==============================] - 0s 158us/step - loss: 0.0142\n",
      "Epoch 71/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0141\n",
      "Epoch 72/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0142\n",
      "Epoch 73/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0142\n",
      "Epoch 74/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0141\n",
      "Epoch 75/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0141\n",
      "Epoch 76/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0141\n",
      "Epoch 77/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0142\n",
      "Epoch 78/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0141\n",
      "Epoch 79/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 80/500\n",
      "531/531 [==============================] - 0s 152us/step - loss: 0.0141\n",
      "Epoch 81/500\n",
      "531/531 [==============================] - 0s 77us/step - loss: 0.0141\n",
      "Epoch 82/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0141\n",
      "Epoch 83/500\n",
      "531/531 [==============================] - 0s 130us/step - loss: 0.0140\n",
      "Epoch 84/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0141\n",
      "Epoch 85/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0141\n",
      "Epoch 86/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0141\n",
      "Epoch 87/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 88/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0141\n",
      "Epoch 89/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0141\n",
      "Epoch 90/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0140\n",
      "Epoch 91/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 92/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 93/500\n",
      "531/531 [==============================] - 0s 116us/step - loss: 0.0141\n",
      "Epoch 94/500\n",
      "531/531 [==============================] - 0s 155us/step - loss: 0.0140\n",
      "Epoch 95/500\n",
      "531/531 [==============================] - 0s 168us/step - loss: 0.0140\n",
      "Epoch 96/500\n",
      "531/531 [==============================] - 0s 183us/step - loss: 0.0140\n",
      "Epoch 97/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 99/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 100/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 101/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 102/500\n",
      "531/531 [==============================] - 0s 175us/step - loss: 0.0140\n",
      "Epoch 103/500\n",
      "531/531 [==============================] - 0s 180us/step - loss: 0.0140\n",
      "Epoch 104/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 105/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 106/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 107/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 108/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 109/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 110/500\n",
      "531/531 [==============================] - 0s 155us/step - loss: 0.0140\n",
      "Epoch 111/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 112/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 113/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 114/500\n",
      "531/531 [==============================] - 0s 170us/step - loss: 0.0140\n",
      "Epoch 115/500\n",
      "531/531 [==============================] - 0s 170us/step - loss: 0.0140\n",
      "Epoch 116/500\n",
      "531/531 [==============================] - 0s 170us/step - loss: 0.0140\n",
      "Epoch 117/500\n",
      "531/531 [==============================] - 0s 177us/step - loss: 0.0140\n",
      "Epoch 118/500\n",
      "531/531 [==============================] - 0s 179us/step - loss: 0.0140\n",
      "Epoch 119/500\n",
      "531/531 [==============================] - 0s 170us/step - loss: 0.0140\n",
      "Epoch 120/500\n",
      "531/531 [==============================] - 0s 179us/step - loss: 0.0140\n",
      "Epoch 121/500\n",
      "531/531 [==============================] - 0s 161us/step - loss: 0.0140\n",
      "Epoch 122/500\n",
      "531/531 [==============================] - 0s 135us/step - loss: 0.0140\n",
      "Epoch 123/500\n",
      "531/531 [==============================] - 0s 101us/step - loss: 0.0140\n",
      "Epoch 124/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 125/500\n",
      "531/531 [==============================] - 0s 151us/step - loss: 0.0140\n",
      "Epoch 126/500\n",
      "531/531 [==============================] - 0s 148us/step - loss: 0.0140\n",
      "Epoch 127/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0140\n",
      "Epoch 128/500\n",
      "531/531 [==============================] - 0s 96us/step - loss: 0.0140\n",
      "Epoch 129/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 130/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 131/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 132/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0140\n",
      "Epoch 133/500\n",
      "531/531 [==============================] - 0s 150us/step - loss: 0.0140\n",
      "Epoch 134/500\n",
      "531/531 [==============================] - 0s 109us/step - loss: 0.0140\n",
      "Epoch 135/500\n",
      "531/531 [==============================] - 0s 116us/step - loss: 0.0140\n",
      "Epoch 136/500\n",
      "531/531 [==============================] - 0s 155us/step - loss: 0.0140\n",
      "Epoch 137/500\n",
      "531/531 [==============================] - 0s 161us/step - loss: 0.0140\n",
      "Epoch 138/500\n",
      "531/531 [==============================] - 0s 175us/step - loss: 0.0140\n",
      "Epoch 139/500\n",
      "531/531 [==============================] - 0s 161us/step - loss: 0.0140\n",
      "Epoch 140/500\n",
      "531/531 [==============================] - 0s 176us/step - loss: 0.0140\n",
      "Epoch 141/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 142/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 143/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 144/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 145/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 146/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 147/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 148/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0140\n",
      "Epoch 149/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 150/500\n",
      "531/531 [==============================] - 0s 148us/step - loss: 0.0140\n",
      "Epoch 151/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 152/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 153/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 154/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 155/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 156/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 157/500\n",
      "531/531 [==============================] - 0s 150us/step - loss: 0.0140\n",
      "Epoch 158/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 159/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0140\n",
      "Epoch 160/500\n",
      "531/531 [==============================] - 0s 160us/step - loss: 0.0140\n",
      "Epoch 161/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 162/500\n",
      "531/531 [==============================] - 0s 151us/step - loss: 0.0140\n",
      "Epoch 163/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 164/500\n",
      "531/531 [==============================] - 0s 148us/step - loss: 0.0140\n",
      "Epoch 165/500\n",
      "531/531 [==============================] - 0s 197us/step - loss: 0.0140\n",
      "Epoch 166/500\n",
      "531/531 [==============================] - 0s 176us/step - loss: 0.0140\n",
      "Epoch 167/500\n",
      "531/531 [==============================] - 0s 125us/step - loss: 0.0140\n",
      "Epoch 168/500\n",
      "531/531 [==============================] - 0s 117us/step - loss: 0.0140\n",
      "Epoch 169/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 170/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 171/500\n",
      "531/531 [==============================] - 0s 109us/step - loss: 0.0140\n",
      "Epoch 172/500\n",
      "531/531 [==============================] - 0s 110us/step - loss: 0.0140\n",
      "Epoch 173/500\n",
      "531/531 [==============================] - 0s 118us/step - loss: 0.0140\n",
      "Epoch 174/500\n",
      "531/531 [==============================] - 0s 151us/step - loss: 0.0140\n",
      "Epoch 175/500\n",
      "531/531 [==============================] - 0s 74us/step - loss: 0.0140\n",
      "Epoch 176/500\n",
      "531/531 [==============================] - 0s 97us/step - loss: 0.0140\n",
      "Epoch 177/500\n",
      "531/531 [==============================] - 0s 95us/step - loss: 0.0140\n",
      "Epoch 178/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 179/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 180/500\n",
      "531/531 [==============================] - 0s 128us/step - loss: 0.0140\n",
      "Epoch 181/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 182/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 183/500\n",
      "531/531 [==============================] - 0s 106us/step - loss: 0.0140\n",
      "Epoch 184/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 185/500\n",
      "531/531 [==============================] - 0s 135us/step - loss: 0.0140\n",
      "Epoch 186/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 187/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 188/500\n",
      "531/531 [==============================] - 0s 101us/step - loss: 0.0140\n",
      "Epoch 189/500\n",
      "531/531 [==============================] - 0s 83us/step - loss: 0.0140\n",
      "Epoch 190/500\n",
      "531/531 [==============================] - 0s 108us/step - loss: 0.0140\n",
      "Epoch 191/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 192/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 193/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 195/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 196/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 197/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 198/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 199/500\n",
      "531/531 [==============================] - 0s 105us/step - loss: 0.0140\n",
      "Epoch 200/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 201/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 202/500\n",
      "531/531 [==============================] - 0s 156us/step - loss: 0.0140\n",
      "Epoch 203/500\n",
      "531/531 [==============================] - 0s 114us/step - loss: 0.0140\n",
      "Epoch 204/500\n",
      "531/531 [==============================] - 0s 126us/step - loss: 0.0140\n",
      "Epoch 205/500\n",
      "531/531 [==============================] - 0s 162us/step - loss: 0.0140\n",
      "Epoch 206/500\n",
      "531/531 [==============================] - 0s 70us/step - loss: 0.0140\n",
      "Epoch 207/500\n",
      "531/531 [==============================] - 0s 97us/step - loss: 0.0140\n",
      "Epoch 208/500\n",
      "531/531 [==============================] - 0s 100us/step - loss: 0.0140\n",
      "Epoch 209/500\n",
      "531/531 [==============================] - 0s 84us/step - loss: 0.0140\n",
      "Epoch 210/500\n",
      "531/531 [==============================] - 0s 104us/step - loss: 0.0140\n",
      "Epoch 211/500\n",
      "531/531 [==============================] - 0s 84us/step - loss: 0.0140\n",
      "Epoch 212/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 213/500\n",
      "531/531 [==============================] - 0s 124us/step - loss: 0.0140\n",
      "Epoch 214/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 215/500\n",
      "531/531 [==============================] - 0s 125us/step - loss: 0.0140\n",
      "Epoch 216/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0140\n",
      "Epoch 217/500\n",
      "531/531 [==============================] - 0s 129us/step - loss: 0.0140\n",
      "Epoch 218/500\n",
      "531/531 [==============================] - 0s 155us/step - loss: 0.0140\n",
      "Epoch 219/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 220/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 221/500\n",
      "531/531 [==============================] - 0s 80us/step - loss: 0.0140\n",
      "Epoch 222/500\n",
      "531/531 [==============================] - 0s 155us/step - loss: 0.0140\n",
      "Epoch 223/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 224/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 225/500\n",
      "531/531 [==============================] - 0s 114us/step - loss: 0.0140\n",
      "Epoch 226/500\n",
      "531/531 [==============================] - 0s 93us/step - loss: 0.0140\n",
      "Epoch 227/500\n",
      "531/531 [==============================] - 0s 91us/step - loss: 0.0140\n",
      "Epoch 228/500\n",
      "531/531 [==============================] - 0s 109us/step - loss: 0.0140\n",
      "Epoch 229/500\n",
      "531/531 [==============================] - 0s 84us/step - loss: 0.0140\n",
      "Epoch 230/500\n",
      "531/531 [==============================] - 0s 92us/step - loss: 0.0140\n",
      "Epoch 231/500\n",
      "531/531 [==============================] - 0s 116us/step - loss: 0.0140\n",
      "Epoch 232/500\n",
      "531/531 [==============================] - 0s 158us/step - loss: 0.0140\n",
      "Epoch 233/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 234/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 235/500\n",
      "531/531 [==============================] - 0s 101us/step - loss: 0.0140\n",
      "Epoch 236/500\n",
      "531/531 [==============================] - 0s 114us/step - loss: 0.0140\n",
      "Epoch 237/500\n",
      "531/531 [==============================] - 0s 78us/step - loss: 0.0140\n",
      "Epoch 238/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 239/500\n",
      "531/531 [==============================] - 0s 135us/step - loss: 0.0140\n",
      "Epoch 240/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 241/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 242/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 243/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 244/500\n",
      "531/531 [==============================] - 0s 100us/step - loss: 0.0140\n",
      "Epoch 245/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 246/500\n",
      "531/531 [==============================] - 0s 71us/step - loss: 0.0140\n",
      "Epoch 247/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 248/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 249/500\n",
      "531/531 [==============================] - 0s 170us/step - loss: 0.0140\n",
      "Epoch 250/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 251/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 252/500\n",
      "531/531 [==============================] - 0s 74us/step - loss: 0.0140\n",
      "Epoch 253/500\n",
      "531/531 [==============================] - 0s 102us/step - loss: 0.0140\n",
      "Epoch 254/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0140\n",
      "Epoch 255/500\n",
      "531/531 [==============================] - 0s 124us/step - loss: 0.0140\n",
      "Epoch 256/500\n",
      "531/531 [==============================] - 0s 121us/step - loss: 0.0140\n",
      "Epoch 257/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 258/500\n",
      "531/531 [==============================] - 0s 114us/step - loss: 0.0140\n",
      "Epoch 259/500\n",
      "531/531 [==============================] - 0s 148us/step - loss: 0.0140\n",
      "Epoch 260/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0140\n",
      "Epoch 261/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0140\n",
      "Epoch 262/500\n",
      "531/531 [==============================] - 0s 135us/step - loss: 0.0140\n",
      "Epoch 263/500\n",
      "531/531 [==============================] - 0s 77us/step - loss: 0.0140\n",
      "Epoch 264/500\n",
      "531/531 [==============================] - 0s 123us/step - loss: 0.0140\n",
      "Epoch 265/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 266/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 267/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 268/500\n",
      "531/531 [==============================] - 0s 104us/step - loss: 0.0140\n",
      "Epoch 269/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 270/500\n",
      "531/531 [==============================] - 0s 158us/step - loss: 0.0140\n",
      "Epoch 271/500\n",
      "531/531 [==============================] - 0s 126us/step - loss: 0.0140\n",
      "Epoch 272/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 273/500\n",
      "531/531 [==============================] - 0s 74us/step - loss: 0.0140\n",
      "Epoch 274/500\n",
      "531/531 [==============================] - 0s 95us/step - loss: 0.0140\n",
      "Epoch 275/500\n",
      "531/531 [==============================] - 0s 125us/step - loss: 0.0140\n",
      "Epoch 276/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 277/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 278/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 279/500\n",
      "531/531 [==============================] - 0s 106us/step - loss: 0.0140\n",
      "Epoch 280/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 281/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 282/500\n",
      "531/531 [==============================] - 0s 73us/step - loss: 0.0140\n",
      "Epoch 283/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 284/500\n",
      "531/531 [==============================] - 0s 97us/step - loss: 0.0140\n",
      "Epoch 285/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 286/500\n",
      "531/531 [==============================] - 0s 98us/step - loss: 0.0140\n",
      "Epoch 287/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 288/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 289/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 291/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 292/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 293/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 294/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 295/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 296/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 297/500\n",
      "531/531 [==============================] - 0s 92us/step - loss: 0.0140\n",
      "Epoch 298/500\n",
      "531/531 [==============================] - 0s 94us/step - loss: 0.0140\n",
      "Epoch 299/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 300/500\n",
      "531/531 [==============================] - 0s 92us/step - loss: 0.0140\n",
      "Epoch 301/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 302/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 303/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 304/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 305/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 306/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 307/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 308/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 309/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 310/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 311/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 312/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 313/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 314/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 315/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 316/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 317/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 318/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 319/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 320/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 321/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 322/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 323/500\n",
      "531/531 [==============================] - 0s 153us/step - loss: 0.0140\n",
      "Epoch 324/500\n",
      "531/531 [==============================] - 0s 68us/step - loss: 0.0140\n",
      "Epoch 325/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 326/500\n",
      "531/531 [==============================] - 0s 113us/step - loss: 0.0140\n",
      "Epoch 327/500\n",
      "531/531 [==============================] - 0s 84us/step - loss: 0.0140\n",
      "Epoch 328/500\n",
      "531/531 [==============================] - 0s 91us/step - loss: 0.0140\n",
      "Epoch 329/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 330/500\n",
      "531/531 [==============================] - 0s 77us/step - loss: 0.0140\n",
      "Epoch 331/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 332/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 333/500\n",
      "531/531 [==============================] - 0s 123us/step - loss: 0.0140\n",
      "Epoch 334/500\n",
      "531/531 [==============================] - 0s 157us/step - loss: 0.0140\n",
      "Epoch 335/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 336/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 337/500\n",
      "531/531 [==============================] - 0s 98us/step - loss: 0.0140\n",
      "Epoch 338/500\n",
      "531/531 [==============================] - 0s 134us/step - loss: 0.0140\n",
      "Epoch 339/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 340/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 341/500\n",
      "531/531 [==============================] - 0s 113us/step - loss: 0.0140\n",
      "Epoch 342/500\n",
      "531/531 [==============================] - 0s 130us/step - loss: 0.0140\n",
      "Epoch 343/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 344/500\n",
      "531/531 [==============================] - 0s 99us/step - loss: 0.0140\n",
      "Epoch 345/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 346/500\n",
      "531/531 [==============================] - 0s 166us/step - loss: 0.0140\n",
      "Epoch 347/500\n",
      "531/531 [==============================] - 0s 134us/step - loss: 0.0140\n",
      "Epoch 348/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 349/500\n",
      "531/531 [==============================] - 0s 151us/step - loss: 0.0140\n",
      "Epoch 350/500\n",
      "531/531 [==============================] - 0s 115us/step - loss: 0.0140\n",
      "Epoch 351/500\n",
      "531/531 [==============================] - 0s 82us/step - loss: 0.0140\n",
      "Epoch 352/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 353/500\n",
      "531/531 [==============================] - 0s 91us/step - loss: 0.0140\n",
      "Epoch 354/500\n",
      "531/531 [==============================] - 0s 124us/step - loss: 0.0140\n",
      "Epoch 355/500\n",
      "531/531 [==============================] - 0s 135us/step - loss: 0.0140\n",
      "Epoch 356/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 357/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 358/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 359/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 360/500\n",
      "531/531 [==============================] - 0s 163us/step - loss: 0.0140\n",
      "Epoch 361/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 362/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 363/500\n",
      "531/531 [==============================] - 0s 71us/step - loss: 0.0140\n",
      "Epoch 364/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 365/500\n",
      "531/531 [==============================] - 0s 120us/step - loss: 0.0140\n",
      "Epoch 366/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 367/500\n",
      "531/531 [==============================] - 0s 151us/step - loss: 0.0140\n",
      "Epoch 368/500\n",
      "531/531 [==============================] - 0s 152us/step - loss: 0.0140\n",
      "Epoch 369/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 370/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 371/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 372/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 373/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 374/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 375/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 376/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0140\n",
      "Epoch 377/500\n",
      "531/531 [==============================] - 0s 115us/step - loss: 0.0140\n",
      "Epoch 378/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 379/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 380/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 381/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 382/500\n",
      "531/531 [==============================] - 0s 166us/step - loss: 0.0140\n",
      "Epoch 383/500\n",
      "531/531 [==============================] - 0s 175us/step - loss: 0.0140\n",
      "Epoch 384/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0140\n",
      "Epoch 385/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/500\n",
      "531/531 [==============================] - 0s 137us/step - loss: 0.0140\n",
      "Epoch 387/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 388/500\n",
      "531/531 [==============================] - 0s 157us/step - loss: 0.0140\n",
      "Epoch 389/500\n",
      "531/531 [==============================] - 0s 127us/step - loss: 0.0140\n",
      "Epoch 390/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 391/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 392/500\n",
      "531/531 [==============================] - 0s 158us/step - loss: 0.0140\n",
      "Epoch 393/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 394/500\n",
      "531/531 [==============================] - 0s 160us/step - loss: 0.0140\n",
      "Epoch 395/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 396/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 397/500\n",
      "531/531 [==============================] - 0s 163us/step - loss: 0.0140\n",
      "Epoch 398/500\n",
      "531/531 [==============================] - 0s 121us/step - loss: 0.0140\n",
      "Epoch 399/500\n",
      "531/531 [==============================] - 0s 187us/step - loss: 0.0140\n",
      "Epoch 400/500\n",
      "531/531 [==============================] - 0s 157us/step - loss: 0.0140\n",
      "Epoch 401/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 402/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 403/500\n",
      "531/531 [==============================] - 0s 146us/step - loss: 0.0140\n",
      "Epoch 404/500\n",
      "531/531 [==============================] - 0s 175us/step - loss: 0.0140\n",
      "Epoch 405/500\n",
      "531/531 [==============================] - 0s 177us/step - loss: 0.0140\n",
      "Epoch 406/500\n",
      "531/531 [==============================] - 0s 182us/step - loss: 0.0140\n",
      "Epoch 407/500\n",
      "531/531 [==============================] - 0s 115us/step - loss: 0.0140\n",
      "Epoch 408/500\n",
      "531/531 [==============================] - 0s 81us/step - loss: 0.0140\n",
      "Epoch 409/500\n",
      "531/531 [==============================] - 0s 94us/step - loss: 0.0140\n",
      "Epoch 410/500\n",
      "531/531 [==============================] - 0s 148us/step - loss: 0.0140\n",
      "Epoch 411/500\n",
      "531/531 [==============================] - 0s 113us/step - loss: 0.0140\n",
      "Epoch 412/500\n",
      "531/531 [==============================] - 0s 79us/step - loss: 0.0140\n",
      "Epoch 413/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 414/500\n",
      "531/531 [==============================] - 0s 88us/step - loss: 0.0140\n",
      "Epoch 415/500\n",
      "531/531 [==============================] - 0s 105us/step - loss: 0.0140\n",
      "Epoch 416/500\n",
      "531/531 [==============================] - 0s 112us/step - loss: 0.0140\n",
      "Epoch 417/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 418/500\n",
      "531/531 [==============================] - 0s 75us/step - loss: 0.0140\n",
      "Epoch 419/500\n",
      "531/531 [==============================] - 0s 144us/step - loss: 0.0140\n",
      "Epoch 420/500\n",
      "531/531 [==============================] - 0s 150us/step - loss: 0.0140\n",
      "Epoch 421/500\n",
      "531/531 [==============================] - 0s 159us/step - loss: 0.0140\n",
      "Epoch 422/500\n",
      "531/531 [==============================] - 0s 119us/step - loss: 0.0140\n",
      "Epoch 423/500\n",
      "531/531 [==============================] - 0s 112us/step - loss: 0.0140\n",
      "Epoch 424/500\n",
      "531/531 [==============================] - 0s 157us/step - loss: 0.0140\n",
      "Epoch 425/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 426/500\n",
      "531/531 [==============================] - 0s 97us/step - loss: 0.0140\n",
      "Epoch 427/500\n",
      "531/531 [==============================] - 0s 85us/step - loss: 0.0140\n",
      "Epoch 428/500\n",
      "531/531 [==============================] - 0s 106us/step - loss: 0.0140\n",
      "Epoch 429/500\n",
      "531/531 [==============================] - 0s 105us/step - loss: 0.0140\n",
      "Epoch 430/500\n",
      "531/531 [==============================] - 0s 138us/step - loss: 0.0140\n",
      "Epoch 431/500\n",
      "531/531 [==============================] - 0s 171us/step - loss: 0.0140\n",
      "Epoch 432/500\n",
      "531/531 [==============================] - 0s 172us/step - loss: 0.0140\n",
      "Epoch 433/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 434/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 435/500\n",
      "531/531 [==============================] - 0s 152us/step - loss: 0.0140\n",
      "Epoch 436/500\n",
      "531/531 [==============================] - 0s 150us/step - loss: 0.0140\n",
      "Epoch 437/500\n",
      "531/531 [==============================] - 0s 170us/step - loss: 0.0140\n",
      "Epoch 438/500\n",
      "531/531 [==============================] - 0s 174us/step - loss: 0.0140\n",
      "Epoch 439/500\n",
      "531/531 [==============================] - 0s 74us/step - loss: 0.0140\n",
      "Epoch 440/500\n",
      "531/531 [==============================] - 0s 101us/step - loss: 0.0140\n",
      "Epoch 441/500\n",
      "531/531 [==============================] - 0s 127us/step - loss: 0.0140\n",
      "Epoch 442/500\n",
      "531/531 [==============================] - 0s 73us/step - loss: 0.0140\n",
      "Epoch 443/500\n",
      "531/531 [==============================] - 0s 82us/step - loss: 0.0140\n",
      "Epoch 444/500\n",
      "531/531 [==============================] - 0s 99us/step - loss: 0.0140\n",
      "Epoch 445/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 446/500\n",
      "531/531 [==============================] - 0s 87us/step - loss: 0.0140\n",
      "Epoch 447/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 448/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 449/500\n",
      "531/531 [==============================] - 0s 158us/step - loss: 0.0140\n",
      "Epoch 450/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 451/500\n",
      "531/531 [==============================] - 0s 91us/step - loss: 0.0140\n",
      "Epoch 452/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 453/500\n",
      "531/531 [==============================] - 0s 102us/step - loss: 0.0140\n",
      "Epoch 454/500\n",
      "531/531 [==============================] - 0s 83us/step - loss: 0.0140\n",
      "Epoch 455/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 456/500\n",
      "531/531 [==============================] - 0s 145us/step - loss: 0.0140\n",
      "Epoch 457/500\n",
      "531/531 [==============================] - 0s 163us/step - loss: 0.0140\n",
      "Epoch 458/500\n",
      "531/531 [==============================] - 0s 69us/step - loss: 0.0140\n",
      "Epoch 459/500\n",
      "531/531 [==============================] - 0s 124us/step - loss: 0.0140\n",
      "Epoch 460/500\n",
      "531/531 [==============================] - 0s 124us/step - loss: 0.0140\n",
      "Epoch 461/500\n",
      "531/531 [==============================] - 0s 106us/step - loss: 0.0140\n",
      "Epoch 462/500\n",
      "531/531 [==============================] - 0s 151us/step - loss: 0.0140\n",
      "Epoch 463/500\n",
      "531/531 [==============================] - 0s 166us/step - loss: 0.0140\n",
      "Epoch 464/500\n",
      "531/531 [==============================] - 0s 161us/step - loss: 0.0140\n",
      "Epoch 465/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 466/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 467/500\n",
      "531/531 [==============================] - 0s 143us/step - loss: 0.0140\n",
      "Epoch 468/500\n",
      "531/531 [==============================] - 0s 110us/step - loss: 0.0140\n",
      "Epoch 469/500\n",
      "531/531 [==============================] - 0s 147us/step - loss: 0.0140\n",
      "Epoch 470/500\n",
      "531/531 [==============================] - 0s 133us/step - loss: 0.0140\n",
      "Epoch 471/500\n",
      "531/531 [==============================] - 0s 134us/step - loss: 0.0140\n",
      "Epoch 472/500\n",
      "531/531 [==============================] - 0s 134us/step - loss: 0.0140\n",
      "Epoch 473/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 474/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n",
      "Epoch 475/500\n",
      "531/531 [==============================] - 0s 141us/step - loss: 0.0140\n",
      "Epoch 476/500\n",
      "531/531 [==============================] - 0s 149us/step - loss: 0.0140\n",
      "Epoch 477/500\n",
      "531/531 [==============================] - 0s 148us/step - loss: 0.0140\n",
      "Epoch 478/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 479/500\n",
      "531/531 [==============================] - 0s 131us/step - loss: 0.0140\n",
      "Epoch 480/500\n",
      "531/531 [==============================] - 0s 140us/step - loss: 0.0140\n",
      "Epoch 481/500\n",
      "531/531 [==============================] - 0s 139us/step - loss: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 482/500\n",
      "531/531 [==============================] - 0s 136us/step - loss: 0.0140\n",
      "Epoch 483/500\n",
      "531/531 [==============================] - 0s 107us/step - loss: 0.0140\n",
      "Epoch 484/500\n",
      "531/531 [==============================] - 0s 155us/step - loss: 0.0140\n",
      "Epoch 485/500\n",
      "531/531 [==============================] - 0s 168us/step - loss: 0.0140\n",
      "Epoch 486/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 487/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 488/500\n",
      "531/531 [==============================] - 0s 72us/step - loss: 0.0140\n",
      "Epoch 489/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 490/500\n",
      "531/531 [==============================] - 0s 101us/step - loss: 0.0140\n",
      "Epoch 491/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 492/500\n",
      "531/531 [==============================] - 0s 90us/step - loss: 0.0140\n",
      "Epoch 493/500\n",
      "531/531 [==============================] - 0s 89us/step - loss: 0.0140\n",
      "Epoch 494/500\n",
      "531/531 [==============================] - 0s 86us/step - loss: 0.0140\n",
      "Epoch 495/500\n",
      "531/531 [==============================] - 0s 157us/step - loss: 0.0140\n",
      "Epoch 496/500\n",
      "531/531 [==============================] - 0s 150us/step - loss: 0.0140\n",
      "Epoch 497/500\n",
      "531/531 [==============================] - 0s 129us/step - loss: 0.0140\n",
      "Epoch 498/500\n",
      "531/531 [==============================] - 0s 131us/step - loss: 0.0140\n",
      "Epoch 499/500\n",
      "531/531 [==============================] - 0s 142us/step - loss: 0.0140\n",
      "Epoch 500/500\n",
      "531/531 [==============================] - 0s 133us/step - loss: 0.0140\n",
      "Pretraining time:  36.71068000793457\n",
      "Pretrained weights are saved to detrac_results/ae_weights.h5\n"
     ]
    }
   ],
   "source": [
    "# setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 2048\n",
    "maxiter = 2e4\n",
    "tol = 0.001\n",
    "save_dir = 'detrac_results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "update_interval = 200\n",
    "pretrain_epochs = 500\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                       distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "#pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "\n",
    "\n",
    "# prepare the DEC model\n",
    "fps = 20\n",
    "n_clusters = train_x.shape[0] // fps\n",
    "print(\"number of clusters is: \" , n_clusters)\n",
    "dec = DEC(dims=[train_x.shape[-1], 500, 500, 2000, 10], n_clusters=n_clusters, init=init)\n",
    "\n",
    "\n",
    "dec.pretrain(x=train_x, y=None, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               1800500   \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 26)                260       \n",
      "=================================================================\n",
      "Total params: 3,073,270\n",
      "Trainable params: 3,073,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Update interval 200\n",
      "Save interval 1.29638671875\n",
      "Initializing cluster centers with k-means.\n",
      "saving model to: detrac_results/DEC_model_0.h5\n",
      "saving model to: detrac_results/DEC_model_2655.h5\n",
      "saving model to: detrac_results/DEC_model_5310.h5\n",
      "saving model to: detrac_results/DEC_model_7965.h5\n",
      "saving model to: detrac_results/DEC_model_10620.h5\n",
      "saving model to: detrac_results/DEC_model_13275.h5\n",
      "saving model to: detrac_results/DEC_model_15930.h5\n",
      "saving model to: detrac_results/DEC_model_18585.h5\n",
      "saving model to: detrac_results/DEC_model_final.h5\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()\n",
    "dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 200\n",
      "Save interval 1.29638671875\n",
      "Initializing cluster centers with k-means.\n",
      "saving model to: detrac_results/DEC_model_0.h5\n",
      "delta_label  0.0 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: detrac_results/DEC_model_final.h5\n"
     ]
    }
   ],
   "source": [
    "# Fit the training data\n",
    "y_pred = dec.fit(train_x, y=None, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give predictions\n",
    "pred_val = dec.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "print(pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOENCODER + KMEANS\n",
    "# We will first train the autoencoder network + k means to train the DEC network\n",
    "# We will see if the DEC network can improve upon the predictions made / compare and contrast by\n",
    "# Selecting 3 frames from each group\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(width*height,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(500, activation='relu')(input_img)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(2000, activation='relu')(encoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(width*height)(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "# Use of autoencoder + kmeans\n",
    "\n",
    "import time\n",
    "\n",
    "encoder = Model(input_img, encoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "start_time = time.time()\n",
    "train_history = autoencoder.fit(train_x, train_x, epochs=500, batch_size=2048, validation_data=(test_x, test_x))\n",
    "print(\"Total time it took to train autoencoder is \", time.time() - start_time, \" seconds\")\n",
    "\n",
    "#from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score\n",
    "\n",
    "pred_auto_train = encoder.predict(train_x)\n",
    "pred_auto = encoder.predict(test_x)\n",
    "\n",
    "print(pred_auto_train.shape)\n",
    "print(pred_auto.shape)\n",
    "\n",
    "km_auto = KMeans(n_jobs=-1, n_clusters=, n_init=20)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "km_auto.fit(pred_auto_train)\n",
    "print(\"Total time it took to train k_means is \", time.time() - start_time, \" seconds\")\n",
    "\n",
    "pred = km_auto.predict(pred_auto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp36",
   "language": "python",
   "name": "pp36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
